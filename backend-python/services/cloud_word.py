"""Utilities para extracci칩n y scoring de frases (matched_phrases) y generaci칩n de payload para nubes de palabras.

Este m칩dulo intenta importar dependencias (spaCy, sentence-transformers, rapidfuzz, sklearn) pero
funciona con heur칤sticas si faltan. Dise침ado para integrarse con `analizador.py`.
"""
from typing import List, Dict, Any, Iterable, Optional, Tuple
import re
import unicodedata
from collections import Counter, defaultdict
import math

# ---- Lazy imports for optional libs ----
_has_spacy = False
_has_transformers = False
_has_rapidfuzz = False
_has_sklearn = False

try:
    import spacy
    _has_spacy = True
except Exception:
    spacy = None

try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity as _cosine_similarity
    _has_transformers = True
except Exception:
    SentenceTransformer = None

try:
    from rapidfuzz import fuzz
    _has_rapidfuzz = True
except Exception:
    fuzz = None

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    _has_sklearn = True
except Exception:
    TfidfVectorizer = None


def _normalize_text(s: str) -> str:
    s = (s or "").strip().lower()
    s = unicodedata.normalize("NFKD", s)
    s = "".join(ch for ch in s if not unicodedata.combining(ch))
    # remove punctuation except internal hyphens
    s = re.sub(r"[^\w\s\-]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def extract_candidate_phrases(text: str, max_ngram: int = 3) -> List[str]:
    """Extrae candidatos de frases desde un texto.

    Intenta usar spaCy para noun_chunks; si no est치 disponible usa n-grams y regex.
    Retorna frases normalizadas y 칰nicas.
    """
    text = text or ""
    norm = _normalize_text(text)
    candidates = []

    if _has_spacy:
        try:
            # cargar spaCy en modo lazy: usa modelo peque침o si no se ha cargado
            if not hasattr(extract_candidate_phrases, "_nlp"):
                try:
                    extract_candidate_phrases._nlp = spacy.load("es_core_news_sm")
                except Exception:
                    print("No se pudo cargar el modelo de spaCy.")
                    # fallback to blank spanish pipeline
                    extract_candidate_phrases._nlp = spacy.blank("es")

            doc = extract_candidate_phrases._nlp(text)
            for nc in doc.noun_chunks:
                s = _normalize_text(nc.text)
                if s:
                    candidates.append(s)
        except Exception:
            pass

    # n-grams fallback/augmentation
    tokens = [t for t in re.findall(r"\w+", norm) if len(t) > 0]
    L = len(tokens)
    for n in range(1, min(max_ngram, 5) + 1):
        for i in range(0, max(0, L - n + 1)):
            gram = " ".join(tokens[i:i + n])
            # filter stop-ish short tokens
            if len(gram) > 0:
                candidates.append(gram)

    # deduplicate preserving order
    seen = set()
    out = []
    for c in candidates:
        if c in seen:
            continue
        seen.add(c)
        out.append(c)

    return out


def precompute_embeddings_for_phrases(category_phrases: Dict[str, List[str]], model_name: str = "all-MiniLM-L6-v2") -> Dict[str, Any]:
    """Precomputar embeddings para la lista de frases por categor칤a.

    Retorna dict: {category: {'phrases': [...], 'embeddings': np.array}}
    Si sentence-transformers no est치 disponible devuelve None.
    """
    if not _has_transformers:
        return {}

    model = SentenceTransformer(model_name)
    result = {}
    for cat, phrases in category_phrases.items():
        normalized = [_normalize_text(p) for p in phrases if p]
        if not normalized:
            result[cat] = {"phrases": [], "embeddings": None}
            continue
        embeddings = model.encode(normalized, convert_to_numpy=True)
        result[cat] = {"phrases": normalized, "embeddings": embeddings}
    return result


def _fuzzy_score(a: str, b: str) -> float:
    if not _has_rapidfuzz:
        # simple fallback: exact match -> 100, prefix -> 80, substring -> 60
        if a == b:
            return 100.0
        if a in b or b in a:
            return 80.0
        return 0.0
    try:
        return float(fuzz.token_sort_ratio(a, b))
    except Exception:
        return 0.0


def _embedding_similarity_score(vecs_cat, vec_candidate) -> List[float]:
    try:
        sims = _cosine_similarity(vec_candidate.reshape(1, -1), vecs_cat)
        # return flattened list
        return [float(x) for x in sims[0]]
    except Exception:
        return []


def _phrase_exists_in_categories(phrase: str, category_phrases: Dict[str, List[str]], similarity_threshold: float = 0.8) -> bool:
    """
    Verificar si una frase existe en las categor칤as especificadas.

    Args:
        phrase: Frase a verificar
        category_phrases: Diccionario de categor칤as y sus frases
        similarity_threshold: Umbral de similitud para considerar un match v치lido

    Returns:
        bool: True si la frase existe en alguna categor칤a con suficiente similitud
    """
    phrase_norm = _normalize_text(phrase)

    for category, phrases in category_phrases.items():
        for category_phrase in phrases:
            category_phrase_norm = _normalize_text(category_phrase)

            # Verificar coincidencia exacta
            if phrase_norm == category_phrase_norm:
                return True

            # Verificar si una es substring de la otra (para frases compuestas)
            if phrase_norm in category_phrase_norm or category_phrase_norm in phrase_norm:
                return True

            # Verificar similitud fuzzy si est치 disponible
            if _has_rapidfuzz:
                try:
                    similarity = fuzz.token_sort_ratio(phrase_norm, category_phrase_norm) / 100.0
                    if similarity >= similarity_threshold:
                        return True
                except Exception:
                    pass

    return False


def score_candidates(
    candidates: List[str],
    category_phrases: Dict[str, List[str]],
    precomputed_embeddings: Optional[Dict[str, Any]] = None,
    use_embedding: bool = True,
    use_fuzzy: bool = True,
    tfidf_vectorizer: Optional[Any] = None
) -> Dict[str, List[Tuple[str, float, str]]]:
    """Score candidates against each category.

    Returns a dict: {category: [(phrase, score, method), ...]}

    Methods: 'embed', 'fuzzy', 'tfidf' (tfidf only if tfidf_vectorizer provided)
    """
    results: Dict[str, List[Tuple[str, float, str]]] = defaultdict(list)

    # Precompute embeddings for candidates if needed
    candidate_embeddings = None
    if use_embedding and _has_transformers:
        model = getattr(score_candidates, "_embed_model", None)
        if model is None:
            score_candidates._embed_model = SentenceTransformer('all-MiniLM-L6-v2')
            model = score_candidates._embed_model
        candidate_embeddings = model.encode(candidates, convert_to_numpy=True)

    # If TF-IDF vectorizer provided, transform candidates
    candidate_tfidf = None
    if tfidf_vectorizer is not None:
        try:
            candidate_tfidf = tfidf_vectorizer.transform(candidates)
        except Exception:
            candidate_tfidf = None

    for cat, phrases in category_phrases.items():
        # prepare cat embeddings if available
        cat_emb = None
        if precomputed_embeddings and cat in precomputed_embeddings:
            cat_emb = precomputed_embeddings[cat].get("embeddings")

        for idx, cand in enumerate(candidates):
            cand_norm = _normalize_text(cand)
            best_score = 0.0
            best_method = ''

            # VALIDACI칍N PREVIA: Solo procesar si el candidato tiene alguna relaci칩n con las frases de la categor칤a
            if not _has_any_similarity_to_category(cand_norm, phrases):
                continue

            # embedding similarity (max over phrases in category)
            if use_embedding and candidate_embeddings is not None and cat_emb is not None:
                sims = _embedding_similarity_score(cat_emb, candidate_embeddings[idx])
                if sims:
                    maxsim = max(sims)
                    # convert cosine [-1,1] to 0-1, but be more conservative
                    score = max(0.0, float(maxsim))  # Ensure non-negative
                    if score > best_score:
                        best_score = score
                        best_method = 'embed'

            # fuzzy matching
            if use_fuzzy and phrases:
                local_best = 0.0
                for p in phrases:
                    s = _fuzzy_score(cand_norm, _normalize_text(p))
                    if s > local_best:
                        local_best = s
                # normalize fuzzy 0-100 to 0-1
                s_norm = local_best / 100.0
                if s_norm > best_score:
                    best_score = s_norm
                    best_method = 'fuzzy'

            # tfidf similarity (cosine with TF-IDF vectors) - if vectorizer provided and we can transform category phrases
            if tfidf_vectorizer is not None:
                try:
                    # transform category phrases to tfidf and compute cosine with candidate
                    # This is heavier; only do if candidate_tfidf available
                    pass
                except Exception:
                    pass

            # Solo almacenar si tiene un score significativo (> 0)
            if best_score > 0.0:
                results[cat].append((cand, float(best_score), best_method))

        # sort by score desc
        results[cat].sort(key=lambda x: x[1], reverse=True)

    return results


def _has_any_similarity_to_category(candidate_norm: str, phrases: List[str], min_similarity: float = 0.5) -> bool:
    """
    Verificar si un candidato tiene alguna similitud m칤nima con las frases de una categor칤a.

    Args:
        candidate_norm: Candidato normalizado
        phrases: Lista de frases de la categor칤a
        min_similarity: Similitud m칤nima requerida (0.0 a 1.0)

    Returns:
        bool: True si tiene similitud m칤nima con al menos una frase
    """
    if not phrases:
        return False

    for phrase in phrases:
        phrase_norm = _normalize_text(phrase)

        # Verificar coincidencia exacta o substring
        if candidate_norm == phrase_norm:
            return True
        if candidate_norm in phrase_norm or phrase_norm in candidate_norm:
            return True

        # Verificar similitud fuzzy si est치 disponible
        if _has_rapidfuzz:
            try:
                similarity = fuzz.token_sort_ratio(candidate_norm, phrase_norm) / 100.0
                if similarity >= min_similarity:
                    return True
            except Exception:
                pass

    return False


def match_phrases_for_clause(
    text: str,
    category_phrases: Dict[str, List[str]],
    top_n: int = 3,
    precomputed_embeddings: Optional[Dict[str, Any]] = None,
    min_score: float = 0.1,
    max_ngram: int = 3,
    use_embeddings: bool = True
) -> List[Dict[str, Any]]:
    """Extrae candidatos y devuelve las matched_phrases para la cl치usula.

    Args:
        text: Texto de la cl치usula a analizar
        category_phrases: Diccionario de frases por categor칤a
        top_n: N칰mero m치ximo de frases a retornar
        precomputed_embeddings: Embeddings precomputados (opcional)
        min_score: Score m칤nimo para considerar una frase
        max_ngram: M치ximo n-gram para extracci칩n de candidatos
        use_embeddings: Si usar embeddings para similarity

    Devuelve lista de dicts: {'phrase':..., 'score':..., 'method':...}
    """
    # Validar que tenemos categor칤as v치lidas
    if not category_phrases or not any(phrases for phrases in category_phrases.values()):
        print("丘멆잺 No hay categor칤as v치lidas o est치n vac칤as")
        return []

    candidates = extract_candidate_phrases(text, max_ngram=max_ngram)
    if not candidates:
        return []

    scored = score_candidates(
        candidates,
        category_phrases,
        precomputed_embeddings=precomputed_embeddings,
        use_embedding=use_embeddings
    )

    # For each category, pick top candidate; then across categories pick global top_n
    global_hits: List[Tuple[str, float, str]] = []
    for cat, lst in scored.items():
        if not lst:
            continue
        # take best
        phrase, score, method = lst[0]
        if score >= min_score:
            # VALIDACI칍N ADICIONAL: Solo incluir si la frase realmente est치 en alguna categor칤a
            # Usar un umbral m치s estricto para la validaci칩n final
            if _phrase_exists_in_categories(phrase, category_phrases, similarity_threshold=0.85):
                global_hits.append((phrase, score, method))
            else:
                print(f"游뛂 Etiqueta rechazada: '{phrase}' (score: {score:.3f}) - No existe en categor칤as")

    # deduplicate by normalized phrase
    seen = set()
    out = []
    # sort global hits by score desc
    global_hits.sort(key=lambda x: x[1], reverse=True)
    for phrase, score, method in global_hits:
        pnorm = _normalize_text(phrase)
        if pnorm in seen:
            continue
        seen.add(pnorm)
        out.append({"phrase": phrase, "score": float(score), "method": method})
        if len(out) >= top_n:
            break

    return out


def build_wordcloud_payload_from_clauses(clauses: Iterable[Dict[str, Any]], phrase_key: str = 'matched_phrases', min_score: float = 0.0) -> List[Dict[str, Any]]:
    """Agrega matched_phrases desde una colecci칩n de cl치usulas y retorna lista [{text, value}].

    clauses: iterable of dicts como las que devuelve `analizar_contrato_completo` (cada cl치usula puede tener key 'matched_phrases').
    """
    ctr = Counter()
    for c in clauses:
        if not c:
            continue
        mp = c.get(phrase_key) if isinstance(c, dict) else None
        if not mp:
            continue
        for entry in mp:
            if isinstance(entry, str):
                phrase = _normalize_text(entry)
                score = 1.0
            elif isinstance(entry, dict):
                phrase = _normalize_text(entry.get('phrase', '') or '')
                try:
                    score = float(entry.get('score', 0.0) or 0.0)
                except Exception:
                    score = 0.0
            else:
                continue
            if not phrase:
                continue
            if score < min_score:
                continue
            # increment by 1 occurrence (could weight by score)
            ctr[phrase] += 1

    return [{"text": k, "value": v} for k, v in ctr.most_common()]


if __name__ == '__main__':
    # small demo if run directly
    sample = "El contratista recibir치 un anticipo del 20% y un certificado provisional de pago. Se retendr치 un porcentaje como garant칤a."
    categories = {
        'Pago': ['anticipo', 'certificado provisional de pago', 'retenci칩n', 'certificado final de pago'],
        'Penalidades': ['multa', 'penalidad', 'resoluci칩n']
    }

    hits = match_phrases_for_clause(sample, categories, top_n=3)
    print('Hits:', hits)
    wc = build_wordcloud_payload_from_clauses([{'matched_phrases': hits}])
    print('Wordcloud payload:', wc)
